{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Analysis Model Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Zoe Cooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import enchant\n",
    "import pprint as pprint\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from functools import partial\n",
    "from json import JSONDecoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC #svm\n",
    "ps = PorterStemmer()\n",
    "from sklearn import preprocessing\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from vecstack import stacking\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_multiclass_classif_error_report(y_test, preds):\n",
    "    print('Accuracy: ' + str(accuracy_score(y_test, preds)))\n",
    "    print('Avg. F1 (Micro): ' + str(f1_score(y_test, preds, average='micro')))\n",
    "    print('Avg. F1 (Macro): ' + str(f1_score(y_test, preds, average='macro')))\n",
    "    print('Avg. F1 (Weighted): ' + str(f1_score(y_test, preds, average='weighted')))\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\\n\" + str(confusion_matrix(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are an excellent way to improve predictive performance on your machine learning problems.\n",
    "Stacked Generalization or stacking is an ensemble technique that uses a new model to learn how to best combine the predictions from two or more models trained on your dataset.\n",
    "-meta learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its very difficult to distinguish between ratings varying by only 1 star. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read to file\n",
    "file = []\n",
    "for line in open(\"yelp_academic_dataset_review.json\", 'r'):\n",
    "    #data=json.load(f)\n",
    "    file.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making dataframe\n",
    "df = pd.DataFrame(file)\n",
    "\n",
    "ones = df[df['stars'] == 1].sample(n=4000, random_state=1)\n",
    "twos = df[df['stars'] == 2].sample(n=4000, random_state=1)\n",
    "twees = df[df['stars'] == 3].sample(n=4000, random_state=1)\n",
    "fours = df[df['stars'] == 4].sample(n=4000, random_state=1)\n",
    "fives = df[df['stars'] == 5].sample(n=4000, random_state=1)\n",
    "\n",
    "newdf = pd.concat([ones, twos, twees, fours, fives])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping other columns:\n",
    "olddata = df.drop(['business_id', 'date', 'review_id', 'type', 'user_id', 'votes'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Defining preprocess function to clean up 'text' column:\n",
    "def preprocess(raw_text):\n",
    "    #remove punctuaction and put all lowercase\n",
    "    text = raw_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = text.lower()\n",
    "    #tokenize into Unigrams\n",
    "    wordlist = text.split() #turns string into list #could use word tokenize w/ nltk too\n",
    "    #remove non-english words\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    texts = []\n",
    "    #stem words\n",
    "    for word in wordlist:\n",
    "        if d.check(word):\n",
    "            texts.append(ps.stem(word))\n",
    "    #remove stop words\n",
    "    texts = [word for word in texts if word not in stopwords.words(\"english\")]\n",
    "    text = ' '.join(texts) #convert back to string\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "#setting y\n",
    "\n",
    "\n",
    "y=newdf['stars']\n",
    "\n",
    "print(y.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting df column as corpus\n",
    "raw_corpus = newdf['text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing each review in corpus, above is function for that:\n",
    "l = []\n",
    "for review in raw_corpus:\n",
    "    l.append(preprocess(review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtm with pandas and sklearn\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(l)\n",
    "df = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf matrix\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(l)\n",
    "df2 = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "4000\n",
      "16000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df2,y,test_size=0.2) #20 percent is test set\n",
    "print(len(x_train))\n",
    "print(len(x_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below makes predictions on n-folds of train and test dataset. It returns the predictions for train and test for each model. This way we can use them with the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking function\n",
    "def Stacking(model,train,y,test,n_fold):\n",
    "    folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
    "    test_pred=np.empty((test.shape[0],1),int)\n",
    "    train_pred=np.empty((0,1),int)\n",
    "    for train_indices,val_indices in folds.split(train,y.values):\n",
    "        x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
    "        y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
    "        model.fit(X=x_train,y=y_train)\n",
    "        train_pred=np.append(train_pred,model.predict(x_val))\n",
    "        test_pred=np.append(test_pred,model.predict(test))\n",
    "    return test_pred,train_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two base models â€“ svc and kNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearSVC()\n",
    "\n",
    "\n",
    "test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=x_train,test=x_test,y=y_train)\n",
    "\n",
    "train_pred1=pd.DataFrame(train_pred1)\n",
    "test_pred1=pd.DataFrame(test_pred1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = KNeighborsClassifier()\n",
    "#XGBClassifier(random_state=1, n_jobs=-1, learning_rate=0.1, n_estimators=100, max_depth=3)\n",
    "\n",
    "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=x_train,test=x_test,y=y_train)\n",
    "\n",
    "train_pred2=pd.DataFrame(train_pred2)\n",
    "test_pred2=pd.DataFrame(test_pred2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a third model, naive bayes, on the predictions of the svm and xgb models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
    "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
    "\n",
    "#model = LogisticRegression(random_state=1)\n",
    "model = naive_bayes.GaussianNB()\n",
    "model.fit(df,y_train)\n",
    "#finaldftest = df_test[df_test.notnull()]\n",
    "\n",
    "df_test = df_test.dropna()\n",
    "print(df)\n",
    "print(df_test)\n",
    "\n",
    "\n",
    "print(y_test)\n",
    "#dataframe = pd.merge([df_test, y_test])\n",
    "\n",
    "print(len(df_test))\n",
    "print(len(y_test))\n",
    "print(model.score(df_test[40000:], y_test))\n",
    "\n",
    "\n",
    "\n",
    "df = df_test[40000:]\n",
    "print(y_test)\n",
    "\n",
    "df.columns = ['test_pred1', 'test_pred2']\n",
    "print(df)\n",
    "\n",
    "df.boxplot()\n",
    "\n",
    "x = test_pred1[40000:]\n",
    "print(x)\n",
    "\n",
    "y = y_test\n",
    "print(y)\n",
    "\n",
    "one = np.random.rand(50) * 5; \n",
    "print(one); \n",
    "\n",
    "two = np.random.rand(20) * 5; \n",
    "print(two); \n",
    "\n",
    "three = np.random.rand(20) * 5; \n",
    "print(three);\n",
    "\n",
    "four = np.random.rand(20) * 5; \n",
    "print(four);\n",
    "\n",
    "five = np.random.rand(20) * 5; \n",
    "print(five);\n",
    "\n",
    "data = [one, two, three, four, five];\n",
    "\n",
    "#make some plots\n",
    "\n",
    "fig1, ax1 = plt.subplots();\n",
    "ax1.set_title('Actual vs Predicted'); \n",
    "ax1.set_xlabel('Actual');\n",
    "ax1.set_ylabel('Predicted');\n",
    "ax1.boxplot(data);\n",
    "plt.xticks([1, 2, 3, 4, 5], [\"One Star\", \"Two Star\", \"Three Star\", \"Four Star\", \"Five Star\"]);\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is a crude metricâ€“there are of course finer-grained evaluation methods.Itâ€™s likely that some classes are â€˜easierâ€™ to predict than others, so we want to look at how well the classifier can predict each class (for example, only 5-star reviews) individually. Focus on precision and recall. We can measure both at the same time using an F1 Score, which measures both as a single metric. It seems like it's easiest to predict a '4' here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With five rating classes, random guessing would be correct only 20% of the time. Next time... predicting a positive or negative review could be easier, more sentiment analysis with binary classification. Accuracy would probably be a lot higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [0, 0]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [0, 0]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-b922e433244f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m   6663\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[1;32m   6664\u001b[0m                        \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6665\u001b[0;31m                        observed=observed, **kwargs)\n\u001b[0m\u001b[1;32m   6666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6667\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                     \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                                                     mutated=self.mutated)\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m   3289\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3290\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3291\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3292\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0'"
     ]
    }
   ],
   "source": [
    "# Box and Whisker Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "\n",
    "df_test = df_test[40000:]\n",
    "\n",
    "print(df_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = pd.DataFrame(preds)\n",
    "preds.plot(kind='box', subplots=True, layout=(1,1), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
